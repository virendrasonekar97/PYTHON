{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaRe1rUFLPFl34zdkY5bo5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virendrasonekar97/PYTHON/blob/main/copy_of_untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "WYPBzFqYVRaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "Question: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "\n",
        "Answer:\n",
        "A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works by splitting the dataset into branches based on feature values, making decisions at each node to separate data into classes. Each branch represents a choice between alternatives, and leaves of the tree represent decision outcomes or class labels. This approach simplifies complex decision-making processes by visualizing them as a tree structure, making it easy to interpret and explain classifications.\n",
        "\n",
        "\n",
        "\n",
        "Question 2\n",
        "Question: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "Answer:\n",
        "Gini Impurity and Entropy are metrics to measure how mixed the classes are in a node. Gini Impurity checks how often a randomly chosen element from the set would be incorrectly labeled if randomly chosen according to the distribution of labels. Entropy measures the amount of uncertainty or randomness in the data. Lower impurity means better splits, so these measures guide the tree in selecting splits that make nodes purer, resulting in better overall classification performance.\n",
        "\n",
        "\n",
        "\n",
        "Question 3\n",
        "Question: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "\n",
        "Answer:\n",
        "Pre-Pruning limits the growth of the tree by setting constraints like maximum depth during training. The advantage is it prevents overfitting and keeps the model simple. Post-Pruning allows the tree to grow fully, then removes unnecessary branches after training. It helps enhance real-world performance by removing parts that do not improve accuracy or generalization.\n",
        "\n",
        "\n",
        "\n",
        "Question 4\n",
        "Question: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "\n",
        "Answer:\n",
        "Information Gain measures the effectiveness of an attribute in classifying data. It calculates the reduction in entropy achieved by splitting a node based on a particular feature. The feature with the highest Information Gain is chosen for the split, leading to more informative and effective separation of data at each step.\n",
        "\n",
        "\n",
        "\n",
        "Question 5\n",
        "Question: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "Answer:\n",
        "Real-world applications include medical diagnostics, customer segmentation, credit scoring, and fraud detection. Main advantages: Decision Trees are easy to interpret and visualize. Limitations: They can be prone to overfitting, especially with small datasets, and sensitive to noisy data."
      ],
      "metadata": {
        "id": "g7HVpMlsVoTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6\n",
        "Question: Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "Print the model’s accuracy and feature importances\n",
        "Answer:"
      ],
      "metadata": {
        "id": "21KGOGEMWPCs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHyPcDayVP7y",
        "outputId": "37428467-0a54-4051-ff36-56eac2ab38da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature importances: [0.         0.01333333 0.06405596 0.92261071]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "y_pred = clf.predict(X)\n",
        "print('Accuracy:', accuracy_score(y, y_pred))\n",
        "print('Feature importances:', clf.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will display the accuracy and feature importances for the trained Decision Tree using the Gini criterion."
      ],
      "metadata": {
        "id": "uOHQirEKWZfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7\n",
        "Question: Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Anafqej1WelO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X, y)\n",
        "acc_full = accuracy_score(y, clf_full.predict(X))\n",
        "\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X, y)\n",
        "acc_depth3 = accuracy_score(y, clf_depth3.predict(X))\n",
        "\n",
        "print('Fully-grown tree accuracy:', acc_full)\n",
        "print('max_depth=3 tree accuracy:', acc_depth3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0znkWiw6WjdJ",
        "outputId": "4820bdf8-85f6-41cb-ccb6-813223e1d63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 1.0\n",
            "max_depth=3 tree accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows the accuracy for both the default and max_depth=3 decision trees."
      ],
      "metadata": {
        "id": "SCjgnGmiWm3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8\n",
        "Question: Write a Python program to:\n",
        "\n",
        "Load the Boston Housing Dataset\n",
        "\n",
        "Train a Decision Tree Regressor\n",
        "\n",
        "Print the Mean Squared Error (MSE) and feature importances\n",
        "Answer:"
      ],
      "metadata": {
        "id": "qNBnl8jjWq7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = fetch_california_housing()\n",
        "X, y = boston.data, boston.target\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X, y)\n",
        "y_pred = regressor.predict(X)\n",
        "print('MSE:', mean_squared_error(y, y_pred))\n",
        "print('Feature importances:', regressor.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eykMWLo_WuLo",
        "outputId": "9e89c24f-9200-4a1c-9154-8697d2ef2f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 9.547357273459726e-32\n",
            "Feature importances: [0.52477966 0.05068746 0.05260197 0.02656171 0.03345973 0.1316932\n",
            " 0.09451051 0.08570575]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code outputs the MSE and feature importances for the Decision Tree Regresso"
      ],
      "metadata": {
        "id": "gsSJNNdyW9rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9\n",
        "Question: Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "\n",
        "Print the best parameters and the resulting model accuracy\n",
        "Answer:"
      ],
      "metadata": {
        "id": "S8T8q0IUW-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "params = {'max_depth': [2, 3, 4, 5], 'min_samples_split': [2, 3, 4]}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), params, cv=3)\n",
        "grid.fit(X, y)\n",
        "print('Best parameters:', grid.best_params_)\n",
        "print('Best accuracy:', grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnI3O9VbXBK4",
        "outputId": "5f0874cd-83df-4e37-ddb2-2bfd49f7dbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Best accuracy: 0.9733333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will tune the tree and display the best parameters and accuracy.​\n",
        "\n"
      ],
      "metadata": {
        "id": "u35VG61tXTFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10\n",
        "Question: Step-by-step process for predicting disease using Decision Trees:\n",
        "\n",
        "Handle the missing values\n",
        "\n",
        "Encode the categorical features\n",
        "\n",
        "Train a Decision Tree model\n",
        "\n",
        "Tune its hyperparameters\n",
        "\n",
        "Evaluate its performance\n",
        "\n",
        "Business value in real-world\n",
        "Answer:\n",
        "\n",
        "Impute missing values using methods like mean, median, or specific algorithms.\n",
        "\n",
        "Encode categorical features using methods like one-hot encoding or label encoding.\n",
        "\n",
        "Train a Decision Tree and tune its parameters with GridSearch or similar tools.\n",
        "\n",
        "Evaluate using metrics such as accuracy, ROC-AUC, or F1-score.\n",
        "\n",
        "The business value: Offers rapid, interpretable predictions for disease risk, supports early intervention strategies and data-driven healthcare decision-making."
      ],
      "metadata": {
        "id": "zzOdOSxYXY7J"
      }
    }
  ]
}